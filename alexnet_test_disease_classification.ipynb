{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3193e5c9",
   "metadata": {},
   "source": [
    "#### 1.1. Load the dataset which are in three folders, namely train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a2736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# download the data and unzip it. You should have three sets of data\n",
    "data_dir = 'nail_diseases'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/validation'\n",
    "# test_dir = data_dir + '/test'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d895c2b",
   "metadata": {},
   "source": [
    "#### 1.3. Data transformation and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f79ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with some data transformation, do not change\n",
    "data_transforms = {\n",
    "    'training': transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomRotation(30),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224,\n",
    "                                                          0.225])]),\n",
    "\n",
    "    'validation': transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(\n",
    "                                          [0.485, 0.456, 0.406],\n",
    "                                          [0.229, 0.224, 0.225])]),\n",
    "\n",
    "    # 'testing': transforms.Compose([transforms.Resize(256),\n",
    "    #                                transforms.CenterCrop(224),\n",
    "    #                                transforms.ToTensor(),\n",
    "    #                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "    #                                                     [0.229, 0.224,\n",
    "    #                                                      0.225])])\n",
    "}\n",
    "# This specifies how many images to process per training/validation\n",
    "batch_size = 256\n",
    "\n",
    "image_datasets = {\n",
    "    'training': datasets.ImageFolder(train_dir,\n",
    "                                     transform=data_transforms['training']),\n",
    "    # 'testing': datasets.ImageFolder(test_dir,\n",
    "    #                                 transform=data_transforms['testing']),\n",
    "    'validation': datasets.ImageFolder(valid_dir,\n",
    "                                       transform=data_transforms['validation'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'training': torch.utils.data.DataLoader(image_datasets['training'],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True),\n",
    "    # 'testing': torch.utils.data.DataLoader(image_datasets['testing'],\n",
    "    #                                        batch_size=batch_size,\n",
    "    #                                        shuffle=False),\n",
    "    'validation': torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "}\n",
    "\n",
    "# get the length of each dataloaders; wit batch size=256, you should have 26 batches of training samples, each with 256 images \n",
    "training_len = len(dataloaders['training'])\n",
    "# test_len = len(dataloaders['testing'])\n",
    "validation_len = len(dataloaders['validation'])\n",
    "class_to_idx = image_datasets['training'].class_to_idx\n",
    "\n",
    "print('We have ', training_len, 'batches of training images;', 'each with',\n",
    "      batch_size, 'images')\n",
    "print('We have ', validation_len, 'batches of validation images;', 'each with',\n",
    "      batch_size, 'images')\n",
    "\n",
    "# Let us preview size of each batch\n",
    "print('Single batch', next(iter(dataloaders['training']))[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8294253",
   "metadata": {},
   "source": [
    "#### 1.4. Initialize the pretrained model\n",
    "\n",
    "We are going to use an Alexnet convolutional neural network as our pretrained model. The architecture of the network is shown in \n",
    "\n",
    " https://bouzouitina-hamdi.medium.com/alexnet-imagenet-classification-with-deep-convolutional-neural-networks-d0210289746b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning with alexnet\n",
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee4907",
   "metadata": {},
   "source": [
    "### 1.5. Update the classifier of the model to be compatible with our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In transfer learning, we freeze the feature parameters from the original model\n",
    "# The most straightforward way is to freeze their gradient update\n",
    "# see https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html for how to set it\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(model)\n",
    "\n",
    "# This is the original classifier with alexnet\n",
    "print()\n",
    "print('Original classifier of Alexnet')\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab214af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to replace the classifier by our datasets\n",
    "# Note, fc2's out_feature should match our number of classes, which is 102\n",
    "\n",
    "# Tip: check the original AlexNet classifier for some thoughts; they are similar.\n",
    "\n",
    "# Defining number of hidden units in our fully connected layer\n",
    "hidden_units = 4096\n",
    "number_of_nail_diseases = len(class_to_idx)\n",
    "\n",
    "# Defining the fully connected layer that will be trained on the flower images\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(9216, hidden_units)), # this should be a Linear layer; how should it be connected to the pretrained model?\n",
    "    ('relu', nn.ReLU()),\n",
    "    ('dropout', nn.Dropout(0.5)),\n",
    "    ('fc2', nn.Linear(hidden_units, number_of_nail_diseases)), # what should be the output of this layer?\n",
    "    ('output', nn.LogSoftmax(dim=1))\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check if the classifier of the model is updated.\n",
    "model.classifier = classifier\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f204dc",
   "metadata": {},
   "source": [
    "### 1.6. Set up the configurations and train it!\n",
    "- epochs = 30\n",
    "- since it is a multiclassification problem, we use [negative log likelihood loss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)\n",
    "- we use the [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)\n",
    "- learning rate (lr) = 0.001\n",
    "\n",
    "In each epoch, we go through all the training samples to update the model, and evaluate with the validation samples to see how good the current model is.\n",
    "- accuracy: mean of the matches in predicted class and the labels\n",
    "- loss: negative log likelihood loss\n",
    "\n",
    "\n",
    "**Sample outputs are provided below; but yours may vary**\n",
    "**Your validation accuracy may be better than train; this is fine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d047311",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 30\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss() # this is the loss\n",
    "print_every = 5\n",
    "\n",
    "validation_accuracies, training_accuracies = [], []\n",
    "validation_losses, training_losses = [], []\n",
    "\n",
    "best_validation_accuracy = 0\n",
    "best_model_weights = None\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    training_loss = training_accuracy = 0\n",
    "    validation_loss = validation_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (images, labels) in enumerate(dataloaders['training']):\n",
    "\n",
    "        # Moving images & labels to the GPU if there is one\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Pushing batch through network, calculating loss & gradient, and updating weights\n",
    "        log_ps = model.forward(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating metrics\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_ps, top_class = ps.topk(1, dim=1)\n",
    "        matches = (top_class == labels.view(*top_class.shape)).type(\n",
    "            torch.FloatTensor)\n",
    "        train_batch_accuracy = matches.mean()\n",
    "\n",
    "        # Resetting optimizer gradient & tracking metrics\n",
    "        optimizer.zero_grad()\n",
    "        training_loss += loss.item()\n",
    "        train_batch_loss = loss.item()\n",
    "        training_accuracy += train_batch_accuracy.item()\n",
    "\n",
    "        if batch_idx % print_every == 0:\n",
    "            # First analyze the training statistics\n",
    "            print('epoch', e, 'batch', batch_idx)\n",
    "            print('training loss per batch', train_batch_loss)\n",
    "            print('training accuracy per batch', train_batch_accuracy.item())\n",
    "\n",
    "    # Then do the analysis for the validation only at the end of each training epochs\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(\n",
    "                dataloaders['validation']):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_ps, top_class = ps.topk(1, dim=1)\n",
    "            matches = (top_class == labels.view(*top_class.shape)).type(\n",
    "                torch.FloatTensor)\n",
    "            validation_batch_accuracy = matches.mean()\n",
    "\n",
    "            # Tracking validation metrics\n",
    "            validation_loss += loss.item()\n",
    "            validation_batch_loss = loss.item()\n",
    "            validation_accuracy += validation_batch_accuracy.item()\n",
    "\n",
    "    # Save the best model weights so far\n",
    "    # Tip: see this https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    if validation_accuracy >= best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_model_weights = model.state_dict()\n",
    "\n",
    "    # Summary statistic per epoch\n",
    "    print('epoch', e, 'summary')\n",
    "    print('avg training loss per epoch', training_loss / training_len)\n",
    "    print('avg training accuracy per epoch', training_accuracy / training_len)\n",
    "    training_losses.append(training_loss / training_len)\n",
    "    training_accuracies.append(training_accuracy / training_len)\n",
    "\n",
    "    print('avg validation loss per epoch', validation_loss / validation_len)\n",
    "    print('avg validation accuracy per epoch',\n",
    "          validation_accuracy / validation_len)\n",
    "    validation_losses.append(validation_loss / validation_len)\n",
    "    validation_accuracies.append(validation_accuracy / validation_len)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Validation Accuracy:', best_validation_accuracy / validation_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd6544",
   "metadata": {},
   "source": [
    "### 1.7. Draw learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Training Summary of Loss\")\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be121134",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Training Summary of Accuracy\")\n",
    "plt.plot(training_accuracies, label='Training Accuracy')\n",
    "plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52573b42",
   "metadata": {},
   "source": [
    "### 1.8. Predict on the test data with the best model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50622d",
   "metadata": {},
   "source": [
    "### Grad-CAM Visualization\n",
    "We compute class-discriminative localization maps using the gradients of the target class flowing into the last convolutional layer (`features[12]`). The map highlights spatial regions most influential for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torchvision.models as models\n",
    "\n",
    "# Grad-CAM storage\n",
    "gradcam_activations = {}\n",
    "gradcam_gradients = {}\n",
    "\n",
    "# Target layer: last convolution layer in AlexNet\n",
    "target_layer = model.features[12]\n",
    "\n",
    "# Forward hook: save activations (feature maps) and register a tensor-level grad hook\n",
    "def _forward_hook(module, inp, out):\n",
    "    # Save detached activations for CAM computation\n",
    "    gradcam_activations['value'] = out.detach()  # shape: (N,C,H,W)\n",
    "    # Register a gradient hook on the tensor output to capture dL/d(activations)\n",
    "    def _tensor_grad_hook(grad):\n",
    "        gradcam_gradients['value'] = grad.detach()  # shape: (N,C,H,W)\n",
    "    out.register_hook(_tensor_grad_hook)\n",
    "\n",
    "# Register hooks\n",
    "# Remove existing handles if this cell is re-run\n",
    "try:\n",
    "    forward_handle.remove()\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    backward_handle.remove()  # in case an old backward handle existed from previous attempts\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "forward_handle = target_layer.register_forward_hook(_forward_hook)\n",
    "\n",
    "\n",
    "def generate_gradcam(model, image_tensor, device, class_index=None):\n",
    "    \"\"\"\n",
    "    Compute Grad-CAM for a single image tensor of shape (1,3,224,224).\n",
    "    If class_index is None, uses the model's top predicted class.\n",
    "    Returns (heatmap, class_index) where heatmap is (H,W) in [0,1].\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    gradcam_activations.clear()\n",
    "    gradcam_gradients.clear()\n",
    "\n",
    "    # Ensure gradients are enabled for this pass (even if model params are frozen)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    image_tensor.requires_grad_(True)\n",
    "\n",
    "    # Forward\n",
    "    with torch.enable_grad():\n",
    "        output = model(image_tensor)  # log-probabilities from final LogSoftmax\n",
    "        if class_index is None:\n",
    "            class_index = int(output.argmax(dim=1).item())\n",
    "        target_score = output[0, class_index]\n",
    "\n",
    "    # Backward to get gradients at target layer\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    target_score.backward(retain_graph=True)\n",
    "\n",
    "    # Retrieve saved activations and gradients\n",
    "    if 'value' not in gradcam_activations or 'value' not in gradcam_gradients:\n",
    "        raise RuntimeError(\"Grad-CAM hooks did not capture activations/gradients.\\n\"\n",
    "                           \"Ensure hooks are registered and gradients are enabled.\")\n",
    "\n",
    "    activations = gradcam_activations['value'].squeeze(0)  # (C,H,W)\n",
    "    gradients = gradcam_gradients['value'].squeeze(0)      # (C,H,W)\n",
    "\n",
    "    # Global average pooling over gradients to get channel weights\n",
    "    weights = gradients.mean(dim=(1, 2))  # (C,)\n",
    "\n",
    "    # Weighted sum of activations\n",
    "    cam = (weights[:, None, None] * activations).sum(dim=0)  # (H,W)\n",
    "\n",
    "    # ReLU and normalize\n",
    "    cam = torch.relu(cam)\n",
    "    cam -= cam.min()\n",
    "    cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "    heatmap = cam.cpu().numpy()\n",
    "    return heatmap, class_index\n",
    "\n",
    "\n",
    "def overlay_heatmap_on_image(heatmap, original_tensor):\n",
    "    \"\"\"\n",
    "    original_tensor: (3,H,W) denormalized image tensor in approx [0,1].\n",
    "    Returns (original_rgb, colored_heatmap_rgb, overlay_rgb) arrays in [0,1].\n",
    "    \"\"\"\n",
    "    img = original_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    colored = cv2.applyColorMap((heatmap_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    colored = cv2.cvtColor(colored, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    overlay = np.clip(0.4 * colored + 0.6 * img, 0, 1)\n",
    "    return img, colored, overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one batch from validation\n",
    "# Create reverse mapping from index to class name\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "val_batch = next(iter(dataloaders['validation']))\n",
    "val_images, val_labels = val_batch\n",
    "\n",
    "# Use first image\n",
    "single_image = val_images[0].unsqueeze(0)  # (1,3,224,224)\n",
    "heatmap, pred_class = generate_gradcam(model, single_image, device)\n",
    "\n",
    "# Get true label\n",
    "true_class = val_labels[0].item()\n",
    "\n",
    "# Get class names\n",
    "true_class_name = idx_to_class[true_class]\n",
    "pred_class_name = idx_to_class[pred_class]\n",
    "\n",
    "# To visualize properly, denormalize image\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "denorm_image = val_images[0]*std + mean\n",
    "\n",
    "orig, colored, overlay = overlay_heatmap_on_image(heatmap, denorm_image)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1); plt.title('Original'); plt.imshow(orig); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.title('Grad-CAM Heatmap'); plt.imshow(heatmap, cmap='jet'); plt.axis('off')\n",
    "plt.subplot(1,3,3); plt.title('Overlay'); plt.imshow(overlay); plt.axis('off')\n",
    "\n",
    "# Color code: green if correct, red if incorrect\n",
    "color = 'green' if pred_class == true_class else 'red'\n",
    "plt.suptitle(f'True: {true_class_name} (idx {true_class}) | Predicted: {pred_class_name} (idx {pred_class})', \n",
    "             color=color, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show grad-CAM visualization for multiple validation images\n",
    "# Create reverse mapping from index to class name\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "num_images = 4\n",
    "val_batch = next(iter(dataloaders['validation']))\n",
    "val_images, val_labels = val_batch\n",
    "\n",
    "for i in range(num_images):\n",
    "    single_image = val_images[i].unsqueeze(0)  # (1,3,224,224)\n",
    "    heatmap, pred_class = generate_gradcam(model, single_image, device)\n",
    "    \n",
    "    # Get true label\n",
    "    true_class = val_labels[i].item()\n",
    "    \n",
    "    # Get class names\n",
    "    true_class_name = idx_to_class[true_class]\n",
    "    pred_class_name = idx_to_class[pred_class]\n",
    "    \n",
    "    # To visualize properly, denormalize image\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    denorm_image = val_images[i]*std + mean\n",
    "\n",
    "    orig, colored, overlay = overlay_heatmap_on_image(heatmap, denorm_image)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); plt.title('Original'); plt.imshow(orig); plt.axis('off')\n",
    "    plt.subplot(1,3,2); plt.title('Grad-CAM Heatmap'); plt.imshow(heatmap, cmap='jet'); plt.axis('off')\n",
    "    plt.subplot(1,3,3); plt.title('Overlay'); plt.imshow(overlay); plt.axis('off')\n",
    "    \n",
    "    # Color code: green if correct, red if incorrect\n",
    "    color = 'green' if pred_class == true_class else 'red'\n",
    "    plt.suptitle(f'True: {true_class_name} (idx {true_class}) | Predicted: {pred_class_name} (idx {pred_class})', \n",
    "                 color=color, fontweight='bold')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Nail Disease Classification)",
   "language": "python",
   "name": "name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
